# 1 "/loongson/ejtag-debug/bin/memtest.c"
# 1 "/loongson/ejtag-debug/bin//"
# 1 "<built-in>"
#define __STDC__ 1
#define __STDC_HOSTED__ 1
#define __GNUC__ 4
#define __GNUC_MINOR__ 4
#define __GNUC_PATCHLEVEL__ 0
#define __SIZE_TYPE__ unsigned int
#define __PTRDIFF_TYPE__ int
#define __WCHAR_TYPE__ int
#define __WINT_TYPE__ unsigned int
#define __INTMAX_TYPE__ long long int
#define __UINTMAX_TYPE__ long long unsigned int
#define __CHAR16_TYPE__ short unsigned int
#define __CHAR32_TYPE__ unsigned int
#define __GXX_ABI_VERSION 1002
#define __SCHAR_MAX__ 127
#define __SHRT_MAX__ 32767
#define __INT_MAX__ 2147483647
#define __LONG_MAX__ 2147483647L
#define __LONG_LONG_MAX__ 9223372036854775807LL
#define __WCHAR_MAX__ 2147483647
#define __CHAR_BIT__ 8
#define __INTMAX_MAX__ 9223372036854775807LL
#define __FLT_EVAL_METHOD__ 0
#define __DEC_EVAL_METHOD__ 2
#define __FLT_RADIX__ 2
#define __FLT_MANT_DIG__ 24
#define __FLT_DIG__ 6
#define __FLT_MIN_EXP__ (-125)
#define __FLT_MIN_10_EXP__ (-37)
#define __FLT_MAX_EXP__ 128
#define __FLT_MAX_10_EXP__ 38
#define __FLT_MAX__ 3.40282347e+38F
#define __FLT_MIN__ 1.17549435e-38F
#define __FLT_EPSILON__ 1.19209290e-7F
#define __FLT_DENORM_MIN__ 1.40129846e-45F
#define __FLT_HAS_DENORM__ 1
#define __FLT_HAS_INFINITY__ 1
#define __FLT_HAS_QUIET_NAN__ 1
#define __DBL_MANT_DIG__ 53
#define __DBL_DIG__ 15
#define __DBL_MIN_EXP__ (-1021)
#define __DBL_MIN_10_EXP__ (-307)
#define __DBL_MAX_EXP__ 1024
#define __DBL_MAX_10_EXP__ 308
#define __DBL_MAX__ 1.7976931348623157e+308
#define __DBL_MIN__ 2.2250738585072014e-308
#define __DBL_EPSILON__ 2.2204460492503131e-16
#define __DBL_DENORM_MIN__ 4.9406564584124654e-324
#define __DBL_HAS_DENORM__ 1
#define __DBL_HAS_INFINITY__ 1
#define __DBL_HAS_QUIET_NAN__ 1
#define __LDBL_MANT_DIG__ 53
#define __LDBL_DIG__ 15
#define __LDBL_MIN_EXP__ (-1021)
#define __LDBL_MIN_10_EXP__ (-307)
#define __LDBL_MAX_EXP__ 1024
#define __LDBL_MAX_10_EXP__ 308
#define __DECIMAL_DIG__ 17
#define __LDBL_MAX__ 1.7976931348623157e+308L
#define __LDBL_MIN__ 2.2250738585072014e-308L
#define __LDBL_EPSILON__ 2.2204460492503131e-16L
#define __LDBL_DENORM_MIN__ 4.9406564584124654e-324L
#define __LDBL_HAS_DENORM__ 1
#define __LDBL_HAS_INFINITY__ 1
#define __LDBL_HAS_QUIET_NAN__ 1
#define __DEC32_MANT_DIG__ 7
#define __DEC32_MIN_EXP__ (-94)
#define __DEC32_MAX_EXP__ 97
#define __DEC32_MIN__ 1E-95DF
#define __DEC32_MAX__ 9.999999E96DF
#define __DEC32_EPSILON__ 1E-6DF
#define __DEC32_SUBNORMAL_MIN__ 0.000001E-95DF
#define __DEC64_MANT_DIG__ 16
#define __DEC64_MIN_EXP__ (-382)
#define __DEC64_MAX_EXP__ 385
#define __DEC64_MIN__ 1E-383DD
#define __DEC64_MAX__ 9.999999999999999E384DD
#define __DEC64_EPSILON__ 1E-15DD
#define __DEC64_SUBNORMAL_MIN__ 0.000000000000001E-383DD
#define __DEC128_MANT_DIG__ 34
#define __DEC128_MIN_EXP__ (-6142)
#define __DEC128_MAX_EXP__ 6145
#define __DEC128_MIN__ 1E-6143DL
#define __DEC128_MAX__ 9.999999999999999999999999999999999E6144DL
#define __DEC128_EPSILON__ 1E-33DL
#define __DEC128_SUBNORMAL_MIN__ 0.000000000000000000000000000000001E-6143DL
#define __SFRACT_FBIT__ 7
#define __SFRACT_IBIT__ 0
#define __SFRACT_MIN__ (-0.5HR-0.5HR)
#define __SFRACT_MAX__ 0X7FP-7HR
#define __SFRACT_EPSILON__ 0x1P-7HR
#define __USFRACT_FBIT__ 8
#define __USFRACT_IBIT__ 0
#define __USFRACT_MIN__ 0.0UHR
#define __USFRACT_MAX__ 0XFFP-8UHR
#define __USFRACT_EPSILON__ 0x1P-8UHR
#define __FRACT_FBIT__ 15
#define __FRACT_IBIT__ 0
#define __FRACT_MIN__ (-0.5R-0.5R)
#define __FRACT_MAX__ 0X7FFFP-15R
#define __FRACT_EPSILON__ 0x1P-15R
#define __UFRACT_FBIT__ 16
#define __UFRACT_IBIT__ 0
#define __UFRACT_MIN__ 0.0UR
#define __UFRACT_MAX__ 0XFFFFP-16UR
#define __UFRACT_EPSILON__ 0x1P-16UR
#define __LFRACT_FBIT__ 31
#define __LFRACT_IBIT__ 0
#define __LFRACT_MIN__ (-0.5LR-0.5LR)
#define __LFRACT_MAX__ 0X7FFFFFFFP-31LR
#define __LFRACT_EPSILON__ 0x1P-31LR
#define __ULFRACT_FBIT__ 32
#define __ULFRACT_IBIT__ 0
#define __ULFRACT_MIN__ 0.0ULR
#define __ULFRACT_MAX__ 0XFFFFFFFFP-32ULR
#define __ULFRACT_EPSILON__ 0x1P-32ULR
#define __LLFRACT_FBIT__ 63
#define __LLFRACT_IBIT__ 0
#define __LLFRACT_MIN__ (-0.5LLR-0.5LLR)
#define __LLFRACT_MAX__ 0X7FFFFFFFFFFFFFFFP-63LLR
#define __LLFRACT_EPSILON__ 0x1P-63LLR
#define __ULLFRACT_FBIT__ 64
#define __ULLFRACT_IBIT__ 0
#define __ULLFRACT_MIN__ 0.0ULLR
#define __ULLFRACT_MAX__ 0XFFFFFFFFFFFFFFFFP-64ULLR
#define __ULLFRACT_EPSILON__ 0x1P-64ULLR
#define __SACCUM_FBIT__ 7
#define __SACCUM_IBIT__ 8
#define __SACCUM_MIN__ (-0X1P7HK-0X1P7HK)
#define __SACCUM_MAX__ 0X7FFFP-7HK
#define __SACCUM_EPSILON__ 0x1P-7HK
#define __USACCUM_FBIT__ 8
#define __USACCUM_IBIT__ 8
#define __USACCUM_MIN__ 0.0UHK
#define __USACCUM_MAX__ 0XFFFFP-8UHK
#define __USACCUM_EPSILON__ 0x1P-8UHK
#define __ACCUM_FBIT__ 15
#define __ACCUM_IBIT__ 16
#define __ACCUM_MIN__ (-0X1P15K-0X1P15K)
#define __ACCUM_MAX__ 0X7FFFFFFFP-15K
#define __ACCUM_EPSILON__ 0x1P-15K
#define __UACCUM_FBIT__ 16
#define __UACCUM_IBIT__ 16
#define __UACCUM_MIN__ 0.0UK
#define __UACCUM_MAX__ 0XFFFFFFFFP-16UK
#define __UACCUM_EPSILON__ 0x1P-16UK
#define __LACCUM_FBIT__ 31
#define __LACCUM_IBIT__ 32
#define __LACCUM_MIN__ (-0X1P31LK-0X1P31LK)
#define __LACCUM_MAX__ 0X7FFFFFFFFFFFFFFFP-31LK
#define __LACCUM_EPSILON__ 0x1P-31LK
#define __ULACCUM_FBIT__ 32
#define __ULACCUM_IBIT__ 32
#define __ULACCUM_MIN__ 0.0ULK
#define __ULACCUM_MAX__ 0XFFFFFFFFFFFFFFFFP-32ULK
#define __ULACCUM_EPSILON__ 0x1P-32ULK
#define __LLACCUM_FBIT__ 31
#define __LLACCUM_IBIT__ 32
#define __LLACCUM_MIN__ (-0X1P31LLK-0X1P31LLK)
#define __LLACCUM_MAX__ 0X7FFFFFFFFFFFFFFFP-31LLK
#define __LLACCUM_EPSILON__ 0x1P-31LLK
#define __ULLACCUM_FBIT__ 32
#define __ULLACCUM_IBIT__ 32
#define __ULLACCUM_MIN__ 0.0ULLK
#define __ULLACCUM_MAX__ 0XFFFFFFFFFFFFFFFFP-32ULLK
#define __ULLACCUM_EPSILON__ 0x1P-32ULLK
#define __QQ_FBIT__ 7
#define __QQ_IBIT__ 0
#define __HQ_FBIT__ 15
#define __HQ_IBIT__ 0
#define __SQ_FBIT__ 31
#define __SQ_IBIT__ 0
#define __DQ_FBIT__ 63
#define __DQ_IBIT__ 0
#define __TQ_FBIT__ 127
#define __TQ_IBIT__ 0
#define __UQQ_FBIT__ 8
#define __UQQ_IBIT__ 0
#define __UHQ_FBIT__ 16
#define __UHQ_IBIT__ 0
#define __USQ_FBIT__ 32
#define __USQ_IBIT__ 0
#define __UDQ_FBIT__ 64
#define __UDQ_IBIT__ 0
#define __UTQ_FBIT__ 128
#define __UTQ_IBIT__ 0
#define __HA_FBIT__ 7
#define __HA_IBIT__ 8
#define __SA_FBIT__ 15
#define __SA_IBIT__ 16
#define __DA_FBIT__ 31
#define __DA_IBIT__ 32
#define __TA_FBIT__ 63
#define __TA_IBIT__ 64
#define __UHA_FBIT__ 8
#define __UHA_IBIT__ 8
#define __USA_FBIT__ 16
#define __USA_IBIT__ 16
#define __UDA_FBIT__ 32
#define __UDA_IBIT__ 32
#define __UTA_FBIT__ 64
#define __UTA_IBIT__ 64
#define __REGISTER_PREFIX__ 
#define __USER_LABEL_PREFIX__ 
#define __VERSION__ "4.4.0"
#define __GNUC_GNU_INLINE__ 1
#define __OPTIMIZE__ 1
#define __FINITE_MATH_ONLY__ 0
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 1
#define __GCC_HAVE_DWARF2_CFI_ASM 1
#define __SIZEOF_INT__ 4
#define __SIZEOF_LONG__ 4
#define __SIZEOF_LONG_LONG__ 8
#define __SIZEOF_SHORT__ 2
#define __SIZEOF_FLOAT__ 4
#define __SIZEOF_DOUBLE__ 8
#define __SIZEOF_LONG_DOUBLE__ 8
#define __SIZEOF_SIZE_T__ 4
#define __SIZEOF_WCHAR_T__ 4
#define __SIZEOF_WINT_T__ 4
#define __SIZEOF_PTRDIFF_T__ 4
#define __SIZEOF_POINTER__ 4
#define __mips__ 1
#define _mips 1
#define mips 1
#define __R3000 1
#define __R3000__ 1
#define R3000 1
#define _R3000 1
#define __mips_fpr 32
#define _MIPS_ARCH_MIPS64R2 1
#define _MIPS_ARCH "mips64r2"
#define _MIPS_TUNE_MIPS64R2 1
#define _MIPS_TUNE "mips64r2"
#define __mips 64
#define __mips_isa_rev 2
#define _MIPS_ISA _MIPS_ISA_MIPS64
#define _ABIO32 1
#define _MIPS_SIM _ABIO32
#define _MIPS_SZINT 32
#define _MIPS_SZLONG 32
#define _MIPS_SZPTR 32
#define _MIPS_FPSET 16
#define __mips_hard_float 1
#define __MIPSEL 1
#define __MIPSEL__ 1
#define MIPSEL 1
#define _MIPSEL 1
#define __LANGUAGE_C 1
#define __LANGUAGE_C__ 1
#define LANGUAGE_C 1
#define _LANGUAGE_C 1
#define __GCC_HAVE_BUILTIN_MIPS_CACHE 1
#define __gnu_linux__ 1
#define __linux 1
#define __linux__ 1
#define linux 1
#define __unix 1
#define __unix__ 1
#define unix 1
#define __ELF__ 1
#define __BIGGEST_ALIGNMENT__ 8
# 1 "<command-line>"
#define GUEST 1
#define CPU_COUNT_PER_US 10
# 1 "include/common.h" 1

#define __COMMON_H__ 
# 1 "include/regdef.h" 1
/*	$OpenBSD: regdef.h,v 1.3 1999/01/27 04:46:06 imp Exp $	*/

/*
 * Copyright (c) 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Ralph Campbell. This file is derived from the MIPS RISC
 * Architecture book by Gerry Kane.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@(#)regdef.h	8.1 (Berkeley) 6/10/93
 */

#define _MIPS_REGDEF_H_ 
# 116 "include/regdef.h"
#define FIFO 0x1F0
#define STACK 0x1E8
#define PARAM 0x1E0
#define PARAMI 0x1E0
#define PARAMO 0x1E0
#define SERIAL 0x1D8
#define FIFO1 0x1D0
#define RTC 0x1C8
#define HEX_ADDR 0x1C0
#define RET_ADDR 0x1B8
#define SPI_IOBASE 0x100
#define LS1DFLASH_IOBASE 0x108

#define DMEM 0x0
# 4 "include/common.h" 2
#define SERIAL_REG (0xffffffffff200000+SERIAL)
#define RTC_REG (0xffffffffff200000+RTC)
#define ARGC_REG (0xffffffffff200000+RET_ADDR)
#define HEX_REG (0xffffffffff200000+HEX_ADDR)

#define ___constant_swab16(x) ((unsigned short)( (((unsigned short)(x) & (unsigned short)0x00ffU) << 8) | (((unsigned short)(x) & (unsigned short)0xff00U) >> 8)))



#define ___constant_swab32(x) ((unsigned int)( (((unsigned int)(x) & (unsigned int)0x000000ffUL) << 24) | (((unsigned int)(x) & (unsigned int)0x0000ff00UL) << 8) | (((unsigned int)(x) & (unsigned int)0x00ff0000UL) >> 8) | (((unsigned int)(x) & (unsigned int)0xff000000UL) >> 24)))





#define ___constant_swab64(x) ((unsigned long long)( (((unsigned long long)(x) & (unsigned long long)0x00000000000000ffULL) << 56) | (((unsigned long long)(x) & (unsigned long long)0x000000000000ff00ULL) << 40) | (((unsigned long long)(x) & (unsigned long long)0x0000000000ff0000ULL) << 24) | (((unsigned long long)(x) & (unsigned long long)0x00000000ff000000ULL) << 8) | (((unsigned long long)(x) & (unsigned long long)0x000000ff00000000ULL) >> 8) | (((unsigned long long)(x) & (unsigned long long)0x0000ff0000000000ULL) >> 24) | (((unsigned long long)(x) & (unsigned long long)0x00ff000000000000ULL) >> 40) | (((unsigned long long)(x) & (unsigned long long)0xff00000000000000ULL) >> 56)))
# 30 "include/common.h"
# 1 "include/stdarg.h" 1

#define __STDARG_H__ 
typedef void * va_list;



#define va_start __builtin_va_start

#define va_arg __builtin_va_arg
#define va_end __builtin_va_end
# 31 "include/common.h" 2
# 1 "include/types.h" 1

#define __TYPE__H__ 
typedef unsigned long long u64;
typedef unsigned int u32;
typedef unsigned short u16;
typedef unsigned char u8;
typedef unsigned int uint;
typedef char __s8;
typedef short __s16;
typedef int __s32;
typedef long long __s64;
typedef char s8;
typedef short s16;
typedef int s32;
typedef long long s64;
typedef unsigned char __u8;
typedef unsigned short __u16;
typedef unsigned int __u32;
typedef unsigned long long __u64;
typedef char int8_t;
typedef short int16_t;
typedef int int32_t;
typedef long long int64_t;
typedef unsigned char uint8_t;
typedef unsigned short uint16_t;
typedef unsigned int uint32_t;
typedef unsigned long long uint64_t;
typedef unsigned char u_int8_t;
typedef unsigned short u_int16_t;
typedef unsigned int u_int32_t;
typedef unsigned long long u_int64_t;
typedef unsigned char u_char;
typedef unsigned int u_int;
typedef unsigned long u_long;
typedef unsigned char __le8;
typedef unsigned short __le16;
typedef unsigned int __le32;
typedef unsigned long long __le64;
typedef unsigned int __be32;
typedef int bool;
enum {
false = 0,
true = 1,
};
typedef unsigned long dma_addr_t;
typedef int gfp_t;
typedef unsigned long uintptr_t;
typedef unsigned long phys_addr_t;
typedef unsigned char uchar;
typedef unsigned short ushort;
typedef unsigned long ulong;

#define __iomem 
#define __packed __attribute__((packed))


#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))
#define BITS_PER_LONG _MIPS_SZLONG
#define BIT(nr) (1UL << (nr))
#define BIT_ULL(nr) (1ULL << (nr))
#define BIT_MASK(nr) (1UL << ((nr) % BITS_PER_LONG))
#define BIT_WORD(nr) ((nr) / BITS_PER_LONG)
#define BIT_ULL_MASK(nr) (1ULL << ((nr) % BITS_PER_LONG_LONG))
#define BIT_ULL_WORD(nr) ((nr) / BITS_PER_LONG_LONG)
#define BITS_PER_BYTE 8
#define BITS_TO_LONGS(nr) DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))
#define DECLARE_BITMAP(name,bits) unsigned long name[BITS_TO_LONGS(bits)]

typedef unsigned long fdt_addr_t;
typedef long ssize_t;
typedef long long off_t;
typedef long long loff_t;
#define SSIZE_MAX LONG_MAX
typedef u_int32_t pcireg_t; /* configuration space register XXX */
typedef unsigned long device_t;
typedef unsigned int mode_t;
typedef unsigned int dev_t;
typedef long intptr_t;
typedef u_int32_t bus_addr_t;
typedef u_int32_t bus_size_t;
typedef u_int32_t bus_space_handle_t;
typedef unsigned long vaddr_t;
typedef char *caddr_t;
typedef unsigned long vm_offset_t;
typedef unsigned int paddr_t;
typedef unsigned int vm_size_t;
typedef unsigned long size_t;
typedef long long quad_t;
typedef unsigned long long u_quad_t; /* quads */
# 32 "include/common.h" 2
#define __P(x) x
#define SSIZE_MAX LONG_MAX

int printf (const char *fmt, ...);
int newprintf (const char *fmt, ...);
int printbase(long v,int w,int base,int sign);
int snprintbase(char *d, int n, long v,int w,int base,int sign);
int sprintbase(char *d, long v,int w,int base,int sign);
int tvsnprintf(char *buf, int n, const char *fmt, void **arg);
int tsnprintf(char *buf, int n, const char *fmt,...);
int tsprintf(char *buf, const char *fmt,...);
#define isdigit(c) (c>='0' && c<='9')

#define NULL ((void *)0)

#define __read_32bit_c0_register(source,sel) ({ int __res; if (sel == 0) __asm__ __volatile__( "mfc0\t%0, " #source "\n\t" : "=r" (__res)); else __asm__ __volatile__( ".set\tmips32\n\t" "mfc0\t%0, " #source ", " #sel "\n\t" ".set\tmips0\n\t" : "=r" (__res)); __res; })
# 63 "include/common.h"
#define __read_64bit_c0_split(source,sel) ({ unsigned long long val; unsigned long flags; local_irq_save(flags); if (sel == 0) __asm__ __volatile__( ".set\tmips64\n\t" "dmfc0\t%M0, " #source "\n\t" "dsll\t%L0, %M0, 32\n\t" "dsrl\t%M0, %M0, 32\n\t" "dsrl\t%L0, %L0, 32\n\t" ".set\tmips0" : "=r" (val)); else __asm__ __volatile__( ".set\tmips64\n\t" "dmfc0\t%M0, " #source ", " #sel "\n\t" "dsll\t%L0, %M0, 32\n\t" "dsrl\t%M0, %M0, 32\n\t" "dsrl\t%L0, %L0, 32\n\t" ".set\tmips0" : "=r" (val)); local_irq_restore(flags); val; })
# 92 "include/common.h"
#define __read_64bit_c0_register(source,sel) ({ unsigned long long __res; if (sizeof(unsigned long) == 4) __res = __read_64bit_c0_split(source, sel); else if (sel == 0) __asm__ __volatile__( ".set\tmips3\n\t" "dmfc0\t%0, " #source "\n\t" ".set\tmips0" : "=r" (__res)); else __asm__ __volatile__( ".set\tmips64\n\t" "dmfc0\t%0, " #source ", " #sel "\n\t" ".set\tmips0" : "=r" (__res)); __res; })
# 113 "include/common.h"
#define __write_32bit_c0_register(register,sel,value) do { if (sel == 0) __asm__ __volatile__( "mtc0\t%z0, " #register "\n\t" : : "Jr" ((unsigned int)(value))); else __asm__ __volatile__( ".set\tmips32\n\t" "mtc0\t%z0, " #register ", " #sel "\n\t" ".set\tmips0" : : "Jr" ((unsigned int)(value))); } while (0)
# 128 "include/common.h"
#define __write_64bit_c0_split(source,sel,val) do { unsigned long flags; local_irq_save(flags); if (sel == 0) __asm__ __volatile__( ".set\tmips64\n\t" "dsll\t%L0, %L0, 32\n\t" "dsrl\t%L0, %L0, 32\n\t" "dsll\t%M0, %M0, 32\n\t" "or\t%L0, %L0, %M0\n\t" "dmtc0\t%L0, " #source "\n\t" ".set\tmips0" : : "r" (val)); else __asm__ __volatile__( ".set\tmips64\n\t" "dsll\t%L0, %L0, 32\n\t" "dsrl\t%L0, %L0, 32\n\t" "dsll\t%M0, %M0, 32\n\t" "or\t%L0, %L0, %M0\n\t" "dmtc0\t%L0, " #source ", " #sel "\n\t" ".set\tmips0" : : "r" (val)); local_irq_restore(flags); } while (0)
# 156 "include/common.h"
#define __write_64bit_c0_register(register,sel,value) do { if (sizeof(unsigned long) == 4) __write_64bit_c0_split(register, sel, value); else if (sel == 0) __asm__ __volatile__( ".set\tmips3\n\t" "dmtc0\t%z0, " #register "\n\t" ".set\tmips0" : : "Jr" (value)); else __asm__ __volatile__( ".set\tmips64\n\t" "dmtc0\t%z0, " #register ", " #sel "\n\t" ".set\tmips0" : : "Jr" (value)); } while (0)
# 175 "include/common.h"
#define __read_ulong_c0_register(reg,sel) ((sizeof(unsigned long) == 4) ? (unsigned long) __read_32bit_c0_register(reg, sel) : (unsigned long) __read_64bit_c0_register(reg, sel))




#define __write_ulong_c0_register(reg,sel,val) do { if (sizeof(unsigned long) == 4) __write_32bit_c0_register(reg, sel, val); else __write_64bit_c0_register(reg, sel, val); } while (0)
# 190 "include/common.h"
#define read_c0_index() __read_32bit_c0_register($0, 0)
#define write_c0_index(val) __write_32bit_c0_register($0, 0, val)

#define read_c0_entrylo0() __read_ulong_c0_register($2, 0)
#define write_c0_entrylo0(val) __write_ulong_c0_register($2, 0, val)

#define read_c0_entrylo1() __read_ulong_c0_register($3, 0)
#define write_c0_entrylo1(val) __write_ulong_c0_register($3, 0, val)

#define read_c0_conf() __read_32bit_c0_register($3, 0)
#define write_c0_conf(val) __write_32bit_c0_register($3, 0, val)

#define read_c0_context() __read_ulong_c0_register($4, 0)
#define write_c0_context(val) __write_ulong_c0_register($4, 0, val)

#define read_c0_pagemask() __read_32bit_c0_register($5, 0)
#define write_c0_pagemask(val) __write_32bit_c0_register($5, 0, val)

#define read_c0_wired() __read_32bit_c0_register($6, 0)
#define write_c0_wired(val) __write_32bit_c0_register($6, 0, val)

#define read_c0_info() __read_32bit_c0_register($7, 0)

#define read_c0_cache() __read_32bit_c0_register($7, 0)
#define write_c0_cache(val) __write_32bit_c0_register($7, 0, val)

#define read_c0_badvaddr() __read_ulong_c0_register($8, 0)
#define write_c0_badvaddr(val) __write_ulong_c0_register($8, 0, val)

#define read_c0_count() __read_32bit_c0_register($9, 0)
#define write_c0_count(val) __write_32bit_c0_register($9, 0, val)

#define read_c0_count2() __read_32bit_c0_register($9, 6)
#define write_c0_count2(val) __write_32bit_c0_register($9, 6, val)

#define read_c0_count3() __read_32bit_c0_register($9, 7)
#define write_c0_count3(val) __write_32bit_c0_register($9, 7, val)

#define read_c0_entryhi() __read_ulong_c0_register($10, 0)
#define write_c0_entryhi(val) __write_ulong_c0_register($10, 0, val)

#define read_c0_compare() __read_32bit_c0_register($11, 0)
#define write_c0_compare(val) __write_32bit_c0_register($11, 0, val)

#define read_c0_compare2() __read_32bit_c0_register($11, 6)
#define write_c0_compare2(val) __write_32bit_c0_register($11, 6, val)

#define read_c0_compare3() __read_32bit_c0_register($11, 7)
#define write_c0_compare3(val) __write_32bit_c0_register($11, 7, val)

#define read_c0_status() __read_32bit_c0_register($12, 0)

#define write_c0_status(val) __write_32bit_c0_register($12, 0, val)


#define read_c0_cause() __read_32bit_c0_register($13, 0)
#define write_c0_cause(val) __write_32bit_c0_register($13, 0, val)

#define read_c0_epc() __read_ulong_c0_register($14, 0)
#define write_c0_epc(val) __write_ulong_c0_register($14, 0, val)

#define read_c0_prid() __read_32bit_c0_register($15, 0)

#define read_c0_config() __read_32bit_c0_register($16, 0)
#define read_c0_config1() __read_32bit_c0_register($16, 1)
#define read_c0_config2() __read_32bit_c0_register($16, 2)
#define read_c0_config3() __read_32bit_c0_register($16, 3)
#define read_c0_config4() __read_32bit_c0_register($16, 4)
#define read_c0_config5() __read_32bit_c0_register($16, 5)
#define read_c0_config6() __read_32bit_c0_register($16, 6)
#define read_c0_config7() __read_32bit_c0_register($16, 7)
#define write_c0_config(val) __write_32bit_c0_register($16, 0, val)
#define write_c0_config1(val) __write_32bit_c0_register($16, 1, val)
#define write_c0_config2(val) __write_32bit_c0_register($16, 2, val)
#define write_c0_config3(val) __write_32bit_c0_register($16, 3, val)
#define write_c0_config4(val) __write_32bit_c0_register($16, 4, val)
#define write_c0_config5(val) __write_32bit_c0_register($16, 5, val)
#define write_c0_config6(val) __write_32bit_c0_register($16, 6, val)
#define write_c0_config7(val) __write_32bit_c0_register($16, 7, val)

/*
 * The WatchLo register.  There may be upto 8 of them.
 */
#define read_c0_watchlo0() __read_ulong_c0_register($18, 0)
#define read_c0_watchlo1() __read_ulong_c0_register($18, 1)
#define read_c0_watchlo2() __read_ulong_c0_register($18, 2)
#define read_c0_watchlo3() __read_ulong_c0_register($18, 3)
#define read_c0_watchlo4() __read_ulong_c0_register($18, 4)
#define read_c0_watchlo5() __read_ulong_c0_register($18, 5)
#define read_c0_watchlo6() __read_ulong_c0_register($18, 6)
#define read_c0_watchlo7() __read_ulong_c0_register($18, 7)
#define write_c0_watchlo0(val) __write_ulong_c0_register($18, 0, val)
#define write_c0_watchlo1(val) __write_ulong_c0_register($18, 1, val)
#define write_c0_watchlo2(val) __write_ulong_c0_register($18, 2, val)
#define write_c0_watchlo3(val) __write_ulong_c0_register($18, 3, val)
#define write_c0_watchlo4(val) __write_ulong_c0_register($18, 4, val)
#define write_c0_watchlo5(val) __write_ulong_c0_register($18, 5, val)
#define write_c0_watchlo6(val) __write_ulong_c0_register($18, 6, val)
#define write_c0_watchlo7(val) __write_ulong_c0_register($18, 7, val)

/*
 * The WatchHi register.  There may be upto 8 of them.
 */
#define read_c0_watchhi0() __read_32bit_c0_register($19, 0)
#define read_c0_watchhi1() __read_32bit_c0_register($19, 1)
#define read_c0_watchhi2() __read_32bit_c0_register($19, 2)
#define read_c0_watchhi3() __read_32bit_c0_register($19, 3)
#define read_c0_watchhi4() __read_32bit_c0_register($19, 4)
#define read_c0_watchhi5() __read_32bit_c0_register($19, 5)
#define read_c0_watchhi6() __read_32bit_c0_register($19, 6)
#define read_c0_watchhi7() __read_32bit_c0_register($19, 7)

#define write_c0_watchhi0(val) __write_32bit_c0_register($19, 0, val)
#define write_c0_watchhi1(val) __write_32bit_c0_register($19, 1, val)
#define write_c0_watchhi2(val) __write_32bit_c0_register($19, 2, val)
#define write_c0_watchhi3(val) __write_32bit_c0_register($19, 3, val)
#define write_c0_watchhi4(val) __write_32bit_c0_register($19, 4, val)
#define write_c0_watchhi5(val) __write_32bit_c0_register($19, 5, val)
#define write_c0_watchhi6(val) __write_32bit_c0_register($19, 6, val)
#define write_c0_watchhi7(val) __write_32bit_c0_register($19, 7, val)


static inline void tlb_write_indexed(void)
{
 __asm__ __volatile__(
  ".set noreorder\n\t"
  "tlbwi\n\t"
  ".set reorder");
}
void * malloc(size_t nbytes);
void * malloc_align(size_t nbytes, int align);
void * zalloc_align(size_t nbytes, int align);
void free(void *ap);
void * memset(void * s,int c, size_t count);
#define __always_inline inline __attribute__((always_inline))
#define noinline __attribute__((noinline))
#define __deprecated __attribute__((deprecated))
#define __packed __attribute__((packed))
#define __weak __attribute__((weak))
#define __alias(symbol) __attribute__((alias(#symbol)))
#define __attribute_const__ __attribute__((__const__))
#define __maybe_unused __attribute__((unused))
#define __always_unused __attribute__((unused))
#define __aligned(x) __attribute__((aligned(x)))
#define ROUND(a,b) (((a) + (b) - 1) & ~((b) - 1))
# 1 "<command-line>" 2
# 1 "/loongson/ejtag-debug/bin/memtest.c"
# 1 "include/r4kcache.h" 1
/*
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Inline assembly cache operations.
 *
 * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com)
 * Copyright (C) 1997 - 2002 Ralf Baechle (ralf@gnu.org)
 * Copyright (C) 2004 Ralf Baechle (ralf@linux-mips.org)
 */

#define _ASM_R4KCACHE_H 
# 1 "include/bitops.h" 1

#define __BITOPS_H 
static inline int fls(int word)
{
 __asm__(".set mips32;clz %0, %1;.set mips0" : "=r" (word) : "r" (word));
 return 32-word;
}

static int inline ffs(int word)
{
 if(!word)
  return 0;
 return fls(word & -word);
}
# 15 "include/r4kcache.h" 2
#define CKSEG0 0xffffffff80000000ULL

#define __STR(x) #x


#define STR(x) __STR(x)



#define PAGE_SIZE 0x4000



#define cpu_dcache_line_size() current_cpu_data.dcache.linesz


#define cpu_icache_line_size() current_cpu_data.icache.linesz


#define cpu_scache_line_size() current_cpu_data.scache.linesz


struct cache_desc {
 unsigned short linesz; /* Size of line in bytes */
 unsigned short ways; /* Number of ways */
 unsigned short sets; /* Number of lines per set */
 unsigned int waysize; /* Bytes per way */
 unsigned int waybit; /* Bits to select in a cache set */
 unsigned int flags; /* Flags describing cache properties */
};

extern struct cpuinfo_mips {
 struct cache_desc icache; /* Primary I-cache */
 struct cache_desc dcache; /* Primary D or combined I/D cache */
 struct cache_desc scache; /* Secondary cache */
 struct cache_desc tcache; /* Tertiary/split secondary cache */
} current_cpu_data;

static int inline probe_cache(struct cpuinfo_mips *current_cpu_data, int cpucount)
{
int prid, config2, slines, ssets, sways, ssize;
int config1, isets, ilines, iways, isize;
int dsets, dlines, dways, dsize;
int swaybit = 0, iwaybit = 0, dwaybit = 0;
prid = ({ int __res; if (0 == 0) __asm__ __volatile__( "mfc0\t%0, " "$15" "\n\t" : "=r" (__res)); else __asm__ __volatile__( ".set\tmips32\n\t" "mfc0\t%0, " "$15" ", " "0" "\n\t" ".set\tmips0\n\t" : "=r" (__res)); __res; });
config2 = ({ int __res; if (2 == 0) __asm__ __volatile__( "mfc0\t%0, " "$16" "\n\t" : "=r" (__res)); else __asm__ __volatile__( ".set\tmips32\n\t" "mfc0\t%0, " "$16" ", " "2" "\n\t" ".set\tmips0\n\t" : "=r" (__res)); __res; });

if (prid==0x6305||prid==0x00146308||prid==0x146101||prid==146309)
{
slines = 2<<((config2>>4)&15);
ssets = 64<<((config2>>8)&15);
sways = 1+((config2)&15);
ssize = ssets*slines*sways*cpucount;

current_cpu_data->scache.linesz = slines;
current_cpu_data->scache.ways = sways;
current_cpu_data->scache.sets = ssets;
current_cpu_data->scache.waysize = ssize/sways;
current_cpu_data->scache.waybit = swaybit;
}


config1 = ({ int __res; if (1 == 0) __asm__ __volatile__( "mfc0\t%0, " "$16" "\n\t" : "=r" (__res)); else __asm__ __volatile__( ".set\tmips32\n\t" "mfc0\t%0, " "$16" ", " "1" "\n\t" ".set\tmips0\n\t" : "=r" (__res)); __res; });
isets = 64<<((config1>>22)&7);
ilines = 2<<((config1>>19)&7);
iways = 1+((config1>>16)&7);
isize = isets*ilines*iways;
dsets = 64<<((config1>>13)&7);
dlines = 2<<((config1>>10)&7);
dways = 1+((config1>>7)&7);
dsize = dsets*dlines*dways;

if(prid == 0x4220)
{
iwaybit = fls(isize/iways);
dwaybit = fls(dsize/dways);
}

current_cpu_data->icache.linesz = ilines;
current_cpu_data->icache.ways = iways;
current_cpu_data->icache.sets = isets;
current_cpu_data->icache.waysize = isize/iways;
current_cpu_data->icache.waybit = iwaybit;

current_cpu_data->dcache.linesz = dlines;
current_cpu_data->dcache.ways = dways;
current_cpu_data->dcache.sets = dsets;
current_cpu_data->dcache.waysize = dsize/dways;
current_cpu_data->dcache.waybit = dwaybit;

return 0;
}

//#include <asm/asm.h>
# 1 "include/cacheops.h" 1
/*
 * Cache operations for the cache instruction.
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * (C) Copyright 1996, 97, 99, 2002, 03 Ralf Baechle
 * (C) Copyright 1999 Silicon Graphics, Inc.
 */

#define __ASM_CACHEOPS_H 

/*
 * Cache Operations available on all MIPS processors with R4000-style caches
 */
#define Index_Invalidate_I 0x00
#define Index_Writeback_Inv_D 0x01
#define Index_Writeback_Inv_V 0x02
#define Index_Load_Tag_I 0x04
#define Index_Load_Tag_D 0x05
#define Index_Store_Tag_I 0x08
#define Index_Store_Tag_D 0x09



#define Hit_Invalidate_I 0x10

#define Hit_Invalidate_D 0x11
#define Hit_Writeback_Inv_D 0x15

/*
 * R4000-specific cacheops
 */
#define Create_Dirty_Excl_D 0x0d
#define Fill 0x14
#define Hit_Writeback_I 0x18
#define Hit_Writeback_D 0x19

/*
 * R4000SC and R4400SC-specific cacheops
 */
#define Index_Invalidate_SI 0x02
#define Index_Writeback_Inv_SD 0x03
#define Index_Load_Tag_SI 0x06
#define Index_Load_Tag_SD 0x07
#define Index_Store_Tag_SI 0x0A
#define Index_Store_Tag_SD 0x0B
#define Create_Dirty_Excl_SD 0x0f
#define Hit_Invalidate_SI 0x12
#define Hit_Invalidate_SD 0x13
#define Hit_Writeback_Inv_SD 0x17
#define Hit_Writeback_SD 0x1b
#define Hit_Set_Virtual_SI 0x1e
#define Hit_Set_Virtual_SD 0x1f

/*
 * R5000-specific cacheops
 */
#define R5K_Page_Invalidate_S 0x17

/*
 * RM7000-specific cacheops
 */
#define Page_Invalidate_T 0x16
#define Index_Store_Tag_T 0x0a
#define Index_Load_Tag_T 0x06

/*
 * R10000-specific cacheops
 *
 * Cacheops 0x02, 0x06, 0x0a, 0x0c-0x0e, 0x16, 0x1a and 0x1e are unused.
 * Most of the _S cacheops are identical to the R4000SC _SD cacheops.
 */
#define Index_Writeback_Inv_S 0x03
#define Index_Load_Tag_S 0x07
#define Index_Store_Tag_S 0x0B
#define Hit_Invalidate_S 0x13
#define Cache_Barrier 0x14
#define Hit_Writeback_Inv_S 0x17
#define Index_Load_Data_I 0x18
#define Index_Load_Data_D 0x19
#define Index_Load_Data_S 0x1b
#define Index_Store_Data_I 0x1c
#define Index_Store_Data_D 0x1d
#define Index_Store_Data_S 0x1f
# 110 "include/r4kcache.h" 2
//#include <asm/cpu-features.h>
//#include <asm/mipsmtregs.h>

/*
 * This macro return a properly sign-extended address suitable as base address
 * for indexed cache operations.  Two issues here:
 *
 *  - The MIPS32 and MIPS64 specs permit an implementation to directly derive
 *    the index bits from the virtual address.  This breaks with tradition
 *    set by the R4000.  To keep unpleasant surprises from happening we pick
 *    an address in KSEG0 / CKSEG0.
 *  - We need a properly sign extended address for 64-bit code.  To get away
 *    without ifdefs we let the compiler do it by a type cast.
 */
#define INDEX_BASE CKSEG0

#define cache_op(op,addr) __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (op), "R" (*(unsigned char *)(addr)))
# 223 "include/r4kcache.h"
#define __iflush_prologue {
#define __iflush_epilogue }
#define __dflush_prologue {
#define __dflush_epilogue }
#define __inv_dflush_prologue {
#define __inv_dflush_epilogue }
#define __sflush_prologue {
#define __sflush_epilogue }
#define __inv_sflush_prologue {
#define __inv_sflush_epilogue }



static inline void flush_icache_line_indexed(unsigned long addr)
{
 {
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x00), "R" (*(unsigned char *)(addr)));
 }
}

static inline void flush_dcache_line_indexed(unsigned long addr)
{
 {
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x01), "R" (*(unsigned char *)(addr)));
 }
}

static inline void flush_scache_line_indexed(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x03), "R" (*(unsigned char *)(addr)));
}

static inline void flush_icache_line(unsigned long addr)
{
 {
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x10), "R" (*(unsigned char *)(addr)));
 }
}

static inline void flush_dcache_line(unsigned long addr)
{
 {
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x15), "R" (*(unsigned char *)(addr)));
 }
}

static inline void invalidate_dcache_line(unsigned long addr)
{
 {
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x11), "R" (*(unsigned char *)(addr)));
 }
}

static inline void invalidate_scache_line(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x13), "R" (*(unsigned char *)(addr)));
}

static inline void flush_scache_line(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x17), "R" (*(unsigned char *)(addr)));
}

#define protected_cache_op(op,addr) __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	"STR(PTR)" 1b, 2b		\n" "	.previous" : : "i" (op), "r" (addr))
# 299 "include/r4kcache.h"
/*
 * The next two are for badland addresses like signal trampolines.
 */
static inline void protected_flush_icache_line(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x10), "r" (addr));
}

/*
 * R10000 / R12000 hazard - these processors don't support the Hit_Writeback_D
 * cacheop so we use Hit_Writeback_Inv_D which is supported by all R4000-style
 * caches.  We're talking about one cacheline unnecessarily getting invalidated
 * here so the penalty isn't overly hard.
 */
static inline void protected_writeback_dcache_line(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x15), "r" (addr));
}

static inline void protected_writeback_scache_line(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x17), "r" (addr));
}

/*
 * This one is RM7000-specific
 */
static inline void invalidate_tcache_page(unsigned long addr)
{
 __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x16), "R" (*(unsigned char *)(addr)));
}

#define cache16_unroll32(base,op) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (base), "i" (op));
# 357 "include/r4kcache.h"
#define cache32_unroll32(base,op) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (base), "i" (op));
# 383 "include/r4kcache.h"
#define cache64_unroll32(base,op) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (base), "i" (op));
# 409 "include/r4kcache.h"
#define cache128_unroll32(base,op) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (base), "i" (op));
# 435 "include/r4kcache.h"
/* build blast_xxx, blast_xxx_page, blast_xxx_page_indexed */
#define __BUILD_BLAST_CACHE(pfx,desc,indexop,hitop,lsize) static inline void blast_ ##pfx ##cache ##lsize(void) { unsigned long start = INDEX_BASE; unsigned long end = start + current_cpu_data.desc.waysize; unsigned long ws_inc = 1UL << current_cpu_data.desc.waybit; unsigned long ws_end = current_cpu_data.desc.ways << current_cpu_data.desc.waybit; unsigned long ws, addr; __ ##pfx ##flush_prologue for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += lsize * 32) cache ##lsize ##_unroll32(addr|ws,indexop); __ ##pfx ##flush_epilogue } static inline void blast_ ##pfx ##cache ##lsize ##_page(unsigned long page) { unsigned long start = page; unsigned long end = page + PAGE_SIZE; __ ##pfx ##flush_prologue do { cache ##lsize ##_unroll32(start,hitop); start += lsize * 32; } while (start < end); __ ##pfx ##flush_epilogue } static inline void blast_ ##pfx ##cache ##lsize ##_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.desc.waysize - 1; unsigned long start = INDEX_BASE + (page & indexmask); unsigned long end = start + PAGE_SIZE; unsigned long ws_inc = 1UL << current_cpu_data.desc.waybit; unsigned long ws_end = current_cpu_data.desc.ways << current_cpu_data.desc.waybit; unsigned long ws, addr; __ ##pfx ##flush_prologue for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += lsize * 32) cache ##lsize ##_unroll32(addr|ws,indexop); __ ##pfx ##flush_epilogue }
# 489 "include/r4kcache.h"
static inline void blast_dcache16(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.dcache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } } static inline void blast_dcache16_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x15));; start += 16 * 32; } while (start < end); } } static inline void blast_dcache16_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.dcache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } }
static inline void blast_icache16(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.icache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } } static inline void blast_icache16_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x10));; start += 16 * 32; } while (start < end); } } static inline void blast_icache16_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.icache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } }
static inline void blast_scache16(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_scache16_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x17));; start += 16 * 32; } while (start < end); } } static inline void blast_scache16_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_dcache32(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.dcache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } } static inline void blast_dcache32_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x15));; start += 32 * 32; } while (start < end); } } static inline void blast_dcache32_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.dcache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } }
static inline void blast_icache32(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.icache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } } static inline void blast_icache32_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x10));; start += 32 * 32; } while (start < end); } } static inline void blast_icache32_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.icache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } }
static inline void blast_scache32(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_scache32_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x17));; start += 32 * 32; } while (start < end); } } static inline void blast_scache32_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_dcache64(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.dcache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } } static inline void blast_dcache64_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x15));; start += 64 * 32; } while (start < end); } } static inline void blast_dcache64_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.dcache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } }
static inline void blast_icache64(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.icache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } } static inline void blast_icache64_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x10));; start += 64 * 32; } while (start < end); } } static inline void blast_icache64_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.icache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.icache.waybit; unsigned long ws_end = current_cpu_data.icache.ways << current_cpu_data.icache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x00));; } }
static inline void blast_scache64(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_scache64_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x17));; start += 64 * 32; } while (start < end); } } static inline void blast_scache64_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_scache128(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 128 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_scache128_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x17));; start += 128 * 32; } while (start < end); } } static inline void blast_scache128_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 128 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }

static inline void blast_inv_dcache16(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.dcache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } } static inline void blast_inv_dcache16_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x11));; start += 16 * 32; } while (start < end); } } static inline void blast_inv_dcache16_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.dcache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } }
static inline void blast_inv_dcache32(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.dcache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } } static inline void blast_inv_dcache32_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x11));; start += 32 * 32; } while (start < end); } } static inline void blast_inv_dcache32_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.dcache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.dcache.waybit; unsigned long ws_end = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x01));; } }
static inline void blast_inv_scache16(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_inv_scache16_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x13));; start += 16 * 32; } while (start < end); } } static inline void blast_inv_scache16_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 16 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x010(%0)	\n" "	cache %1, 0x020(%0); cache %1, 0x030(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x050(%0)	\n" "	cache %1, 0x060(%0); cache %1, 0x070(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x090(%0)	\n" "	cache %1, 0x0a0(%0); cache %1, 0x0b0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0d0(%0)	\n" "	cache %1, 0x0e0(%0); cache %1, 0x0f0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x110(%0)	\n" "	cache %1, 0x120(%0); cache %1, 0x130(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x150(%0)	\n" "	cache %1, 0x160(%0); cache %1, 0x170(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x190(%0)	\n" "	cache %1, 0x1a0(%0); cache %1, 0x1b0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1d0(%0)	\n" "	cache %1, 0x1e0(%0); cache %1, 0x1f0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_inv_scache32(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_inv_scache32_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x13));; start += 32 * 32; } while (start < end); } } static inline void blast_inv_scache32_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 32 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x020(%0)	\n" "	cache %1, 0x040(%0); cache %1, 0x060(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0a0(%0)	\n" "	cache %1, 0x0c0(%0); cache %1, 0x0e0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x120(%0)	\n" "	cache %1, 0x140(%0); cache %1, 0x160(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1a0(%0)	\n" "	cache %1, 0x1c0(%0); cache %1, 0x1e0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x220(%0)	\n" "	cache %1, 0x240(%0); cache %1, 0x260(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2a0(%0)	\n" "	cache %1, 0x2c0(%0); cache %1, 0x2e0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x320(%0)	\n" "	cache %1, 0x340(%0); cache %1, 0x360(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3a0(%0)	\n" "	cache %1, 0x3c0(%0); cache %1, 0x3e0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_inv_scache64(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_inv_scache64_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x13));; start += 64 * 32; } while (start < end); } } static inline void blast_inv_scache64_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 64 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x040(%0)	\n" "	cache %1, 0x080(%0); cache %1, 0x0c0(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x140(%0)	\n" "	cache %1, 0x180(%0); cache %1, 0x1c0(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x240(%0)	\n" "	cache %1, 0x280(%0); cache %1, 0x2c0(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x340(%0)	\n" "	cache %1, 0x380(%0); cache %1, 0x3c0(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x440(%0)	\n" "	cache %1, 0x480(%0); cache %1, 0x4c0(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x540(%0)	\n" "	cache %1, 0x580(%0); cache %1, 0x5c0(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x640(%0)	\n" "	cache %1, 0x680(%0); cache %1, 0x6c0(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x740(%0)	\n" "	cache %1, 0x780(%0); cache %1, 0x7c0(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }
static inline void blast_inv_scache128(void) { unsigned long start = 0xffffffff80000000ULL; unsigned long end = start + current_cpu_data.scache.waysize; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 128 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } } static inline void blast_inv_scache128_page(unsigned long page) { unsigned long start = page; unsigned long end = page + 0x4000; { do { __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (start), "i" (0x13));; start += 128 * 32; } while (start < end); } } static inline void blast_inv_scache128_page_indexed(unsigned long page) { unsigned long indexmask = current_cpu_data.scache.waysize - 1; unsigned long start = 0xffffffff80000000ULL + (page & indexmask); unsigned long end = start + 0x4000; unsigned long ws_inc = 1UL << current_cpu_data.scache.waybit; unsigned long ws_end = current_cpu_data.scache.ways << current_cpu_data.scache.waybit; unsigned long ws, addr; { for (ws = 0; ws < ws_end; ws += ws_inc) for (addr = start; addr < end; addr += 128 * 32) __asm__ __volatile__( "	.set push					\n" "	.set noreorder					\n" "	.set mips3					\n" "	cache %1, 0x000(%0); cache %1, 0x080(%0)	\n" "	cache %1, 0x100(%0); cache %1, 0x180(%0)	\n" "	cache %1, 0x200(%0); cache %1, 0x280(%0)	\n" "	cache %1, 0x300(%0); cache %1, 0x380(%0)	\n" "	cache %1, 0x400(%0); cache %1, 0x480(%0)	\n" "	cache %1, 0x500(%0); cache %1, 0x580(%0)	\n" "	cache %1, 0x600(%0); cache %1, 0x680(%0)	\n" "	cache %1, 0x700(%0); cache %1, 0x780(%0)	\n" "	cache %1, 0x800(%0); cache %1, 0x880(%0)	\n" "	cache %1, 0x900(%0); cache %1, 0x980(%0)	\n" "	cache %1, 0xa00(%0); cache %1, 0xa80(%0)	\n" "	cache %1, 0xb00(%0); cache %1, 0xb80(%0)	\n" "	cache %1, 0xc00(%0); cache %1, 0xc80(%0)	\n" "	cache %1, 0xd00(%0); cache %1, 0xd80(%0)	\n" "	cache %1, 0xe00(%0); cache %1, 0xe80(%0)	\n" "	cache %1, 0xf00(%0); cache %1, 0xf80(%0)	\n" "	.set pop					\n" : : "r" (addr|ws), "i" (0x03));; } }

/* build blast_xxx_range, protected_blast_xxx_range */
#define __BUILD_BLAST_CACHE_RANGE(pfx,desc,hitop,prot) static inline void prot ##blast_ ##pfx ##cache ##_range(unsigned long start, unsigned long end) { unsigned long lsize = cpu_ ##desc ##_line_size(); unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); __ ##pfx ##flush_prologue while (1) { prot ##cache_op(hitop, addr); if (addr == aend) break; addr += lsize; } __ ##pfx ##flush_epilogue }
# 528 "include/r4kcache.h"
static inline void protected_blast_dcache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.dcache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x15), "r" (addr)); if (addr == aend) break; addr += lsize; } } }
static inline void protected_blast_scache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.scache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x17), "r" (addr)); if (addr == aend) break; addr += lsize; } } }
static inline void protected_blast_icache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.icache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push			\n" "	.set	noreorder		\n" "	.set	mips3			\n" "1:	cache	%0, (%1)		\n" "2:	.set	pop			\n" "	.section __ex_table,\"a\"	\n" "	""PTR"" 1b, 2b		\n" "	.previous" : : "i" (0x10), "r" (addr)); if (addr == aend) break; addr += lsize; } } }
static inline void blast_dcache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.dcache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x15), "R" (*(unsigned char *)(addr))); if (addr == aend) break; addr += lsize; } } }
static inline void blast_scache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.scache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x17), "R" (*(unsigned char *)(addr))); if (addr == aend) break; addr += lsize; } } }
/* blast_inv_dcache_range */
static inline void blast_inv_dcache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.dcache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x11), "R" (*(unsigned char *)(addr))); if (addr == aend) break; addr += lsize; } } }
static inline void blast_inv_scache_range(unsigned long start, unsigned long end) { unsigned long lsize = current_cpu_data.scache.linesz; unsigned long addr = start & ~(lsize - 1); unsigned long aend = (end - 1) & ~(lsize - 1); { while (1) { __asm__ __volatile__( "	.set	push					\n" "	.set	noreorder				\n" "	.set	mips3\n\t				\n" "	cache	%0, %1					\n" "	.set	pop					\n" : : "i" (0x13), "R" (*(unsigned char *)(addr))); if (addr == aend) break; addr += lsize; } } }

static inline void r4k_dma_cache_wback_inv(unsigned long addr, unsigned long size)
{
 /* Catch bad driver code */
 int scache_size = current_cpu_data.scache.waysize*current_cpu_data.scache.ways;
 int dcache_size = current_cpu_data.dcache.waysize*current_cpu_data.dcache.ways;

 if(scache_size)
 {
  if (size >= scache_size)
   blast_scache32();
  else
   blast_scache_range(addr, addr + size);
 }
 else
 {
  if (size >= dcache_size)
   blast_dcache32();
  else
   blast_dcache_range(addr, addr + size);
 }
 __sync();
 return;

}

static inline void r4k_dma_cache_inv(unsigned long addr, unsigned long size)
{
 /* Catch bad driver code */
 int scache_size = current_cpu_data.scache.waysize*current_cpu_data.scache.ways;
 int dcache_size = current_cpu_data.dcache.waysize*current_cpu_data.dcache.ways;

 if (scache_size) {
  if (size >= scache_size)
   blast_scache32();
  else
   blast_inv_scache_range(addr, addr + size);
  __sync();
  return;
 }

 if (size >= dcache_size) {
  blast_dcache32();
 } else {
  blast_inv_dcache_range(addr, addr + size);
 }

  __sync();
}
# 2 "/loongson/ejtag-debug/bin/memtest.c" 2
#define SERIALBASE 0xbfe00000
#define printf newprintf

struct cpuinfo_mips current_cpu_data;
# 18 "/loongson/ejtag-debug/bin/memtest.c"
int mymain()
{
 int i;
 long taddr = (int)0x81000000;
 int tsize = 0x100000;

memset(&current_cpu_data,0, sizeof(current_cpu_data));
probe_cache(&current_cpu_data, 2);

for(i=0;i<tsize;i+=64)
{
if((i&0xffff)==0)newprintf("w%08x\n",i);
 *(volatile int *)(taddr + i) = i;
}

//r4k_dma_cache_wback_inv(taddr, tsize);
blast_scache_range(taddr, taddr + tsize);

for(i=0;i<tsize;i+=64)
{
if((i&0xffff)==0)newprintf("r%08x\n",i);
 if(*(volatile int *)(taddr + i) != i) newprintf("verify error on 0x%08x\n");
}
newprintf("done\n");
return 0;
}
